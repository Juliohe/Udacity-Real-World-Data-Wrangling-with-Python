{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Project: Cryptocurrency Analysis (BTC & ETH)\n",
    "\n",
    "## Research Question\n",
    "**How have the prices of Bitcoin (BTC) and Ethereum (ETH) correlated over the last 5 years, and what are their relative trading volumes?**\n",
    "\n",
    "This project analyzes the daily price and volume data of the two largest cryptocurrencies to understand their market relationship.\n",
    "\n",
    "## 1. Gather data\n",
    "\n",
    "In this section, we will gather data from two sources:\n",
    "1.  **Dataset 1**: Bitcoin (BTC) historical data manually downloaded from Yahoo Finance (CSV).\n",
    "2.  **Dataset 2**: Ethereum (ETH) historical data downloaded programmatically using the CoinGecko API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1: Bitcoin (BTC)\n",
    "\n",
    "**Source**: Yahoo Finance\n",
    "**Method**: Manual Download\n",
    "\n",
    "**Instructions:**\n",
    "1.  Go to the Yahoo Finance page for Bitcoin: [https://finance.yahoo.com/quote/BTC-USD/history](https://finance.yahoo.com/quote/BTC-USD/history)\n",
    "2.  Set the **Time Period** to **5 Years** (or the max available if less).\n",
    "3.  Set the **Frequency** to **Daily**.\n",
    "4.  Click **Apply**.\n",
    "5.  Click **Download** to save the CSV file.\n",
    "6.  Rename the downloaded file to `btc_usd.csv`.\n",
    "7.  Create a folder named `Dataset` in your project directory (if it doesn't exist) and move the file there: `Dataset/btc_usd.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Ensure Dataset directory exists\n",
    "if not os.path.exists('Dataset'):\n",
    "    os.makedirs('Dataset')\n",
    "    print(\"Created 'Dataset' directory.\")\n",
    "\n",
    "# Load Dataset 1 (Bitcoin CSV)\n",
    "btc_path = 'Dataset/btc_usd.csv'\n",
    "\n",
    "if os.path.exists(btc_path):\n",
    "    df_btc = pd.read_csv(btc_path)\n",
    "    print(f\"Dataset 1 loaded successfully. Shape: {df_btc.shape}\")\n",
    "    display(df_btc.head())\n",
    "else:\n",
    "    print(f\"File not found: {btc_path}. Please download it manually as per instructions above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2: Ethereum (ETH)\n",
    "\n",
    "**Source**: CoinGecko API\n",
    "**Method**: Programmatic Download (API)\n",
    "\n",
    "We will fetch the last 5 years (approx. 1825 days) of daily market data for Ethereum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# URL for CoinGecko API (Ethereum Market Chart)\n",
    "url = \"https://api.coingecko.com/api/v3/coins/ethereum/market_chart?vs_currency=usd&days=1825&interval=daily\"\n",
    "\n",
    "print(\"Attempting to download Ethereum data from CoinGecko API...\")\n",
    "\n",
    "try:\n",
    "    # Add User-Agent to avoid 403 Forbidden errors\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers, timeout=15)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data_eth = response.json()\n",
    "        print(\"Successfully downloaded data from API.\")\n",
    "\n",
    "        # Save raw JSON for reference\n",
    "        with open('Dataset/eth_data_raw.json', 'w') as f:\n",
    "            json.dump(data_eth, f)\n",
    "\n",
    "        # Process JSON into DataFrame\n",
    "        # CoinGecko returns lists of [timestamp, value]\n",
    "        prices = data_eth.get('prices', [])\n",
    "        volumes = data_eth.get('total_volumes', [])\n",
    "\n",
    "        # Create temporary dataframes\n",
    "        df_prices = pd.DataFrame(prices, columns=['timestamp', 'price'])\n",
    "        df_volumes = pd.DataFrame(volumes, columns=['timestamp', 'volume'])\n",
    "\n",
    "        # Merge on timestamp\n",
    "        df_eth = pd.merge(df_prices, df_volumes, on='timestamp')\n",
    "\n",
    "        # Convert timestamp to datetime (ms to s)\n",
    "        df_eth['Date'] = pd.to_datetime(df_eth['timestamp'], unit='ms')\n",
    "\n",
    "        # Save to CSV\n",
    "        csv_path_eth = 'Dataset/eth_usd.csv'\n",
    "        df_eth.to_csv(csv_path_eth, index=False)\n",
    "        print(f\"Dataset 2 gathered and saved to {csv_path_eth}. Shape: {df_eth.shape}\")\n",
    "        display(df_eth.head())\n",
    "\n",
    "    else:\n",
    "        print(f\"API request failed with status code {response.status_code}\")\n",
    "        print(response.text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"API request failed or encountered an error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assess data\n",
    "\n",
    "Assess the data according to data quality and tidiness metrics.\n",
    "\n",
    "List **two** data quality issues and **two** tidiness issues. Assess each data issue visually **and** programmatically, then briefly describe the issue you find. **Make sure you include justifications for the methods you use for the assessment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Issue 1: Missing Values or Inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue and justification: *FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Issue 2: Data Types (e.g., Date parsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue and justification: *FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness Issue 1: Redundant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue and justification: *FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness Issue 2: Data Structure (Merge requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue and justification: *FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean data\n",
    "\n",
    "Clean the data to solve the 4 issues corresponding to data quality and tidiness found in the assessing step. **Make sure you include justifications for your cleaning decisions.**\n",
    "\n",
    "After the cleaning for each issue, please use **either** the visually or programatical method to validate the cleaning was succesful.\n",
    "\n",
    "At this stage, you are also expected to remove variables that are unnecessary for your analysis and combine your datasets. Depending on your datasets, you may choose to perform variable combination and elimination before or after the cleaning stage. Your dataset must have **at least** 4 variables after combining the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN - Clean Issue 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN - Clean Issue 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN - Remove unnecessary variables and combine datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Update your data store\n",
    "\n",
    "Update your local database/data store with the cleaned data, following best practices for storing your cleaned data:\n",
    "\n",
    "1.  Must maintain different instances of the data (raw and cleaned data)\n",
    "2.  Resourceful naming (e.g. `combined_dataset.csv`)\n",
    "3.  Compare the quantity of raw and cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN - Save the final cleaned dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Answer the research question\n",
    "\n",
    "**4.1:** Define and answer the research question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN - Analysis and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2:** Reflection\n",
    "If I had more time to complete the project, I would..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}